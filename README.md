# Exploring Transformer Interpretability

Transformer models are improving at a rapid pace, making it of paramount importance to develop methods to explain, reverse-engineer, and visualize their inner workings. In this work, we study the interpretability of transformer models through a series of experiments divided into two parts:

1. [Visualizing Transformer Attention](#I.-Visualizing-Transformer-Attention)
2. [Exploring Induction Heads in BERT](#II.-Exploring-Induction-Heads-in-BERT)

This report presents the methods and results of an independent research study conducted over the course of January to April 2023 at the \href{https://insight.seas.harvard.edu/}{Harvard Insight and Interaction Lab} under the direct mentorship of Professor Martin Wattenberg, Professor Fernanda Vi√©gas, and Catherine Yeh. The code for the experiments in this project can be found at this \href{https://github.com/cynthia9chen/interp-research-2023}{Github repository}.


## I. Visualizing Transformer Attention

## II. Exploring Induction Heads in BERT
